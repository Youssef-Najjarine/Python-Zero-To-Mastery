NVIDIA JetPack 6.0 provides optimized builds and containers for a large number of machine learning frameworks like PyTorch and TensorFlow, and now all the LLM and VIT libraries too. There are world-class pre-trained models available to download from TAO, NGC, and Hugging Face that can all be run on Jetpack with unmatched performance, along with edge devices like Jetson.
NVIDIA just released Isaac ROS 2.0 which contains highly optimized vision gems, including SLAM and zero-copy transport between ROS nodes called Nitrous for autonomous robots.
With OWL-ViT you can now enter in the parts of the body you want to detect and track and the software will find and track those body parts.
 Previously you had to pre train the softwares for each and every last detection scenario you want to do.
 When you combine the detections from LVIT with a secondary clip classifier, you can do further semantic analysis on each object.
All the code for LVIT is available on GitHub (github.com/NVIDIA-AI-IOT/nanoowl).
Object detection is by far the most popular computer vision algorithm run.
The segmentation analog part is called SAM (Segment Anything Model).
You provide some control points like clicking on certain parts of the image that you want to segment and it will automatically segment those blobs for you no matter what they are.
It used to be that you would have to manually go and make a segmentation data set, then train the model on that, and those segmentation data sets were very resource intensive to annotate.
Jetson AI Lab is available online, which is a set of tutorials that we've put together that can easily guide you through running LLMs, VITs, VLMs, vector databases, etc...
It's take out a lot of the guesswork and debugging from porting what normally are GitHub projects.
llamaspeak is VERY accurate, fluid, and natural sounding.
laamaspeak could potentially be used to ask to call for help in a hospital setting?
The way that Multimodal embeddings work is essentially they use an embedding model like CLIP that will combine text and images into one common embedding space where contextually concepts are very similar.
Image embeddings can read images and text, audio, inertial measurement units, point clouds, all types of modalities that the LLMs can.
We're essentially enabling the LLMS with all the different senses so they're able to assimilate holistic world view and a perception world model.
LLama can be used for smart traffic intersection, crosswalk monitors, blind assistant devices, baby crib monitors, or anything that is an open-ended problem.
llamaspeak can perfectly analyze photos and describe them to you.
There are two stages to LLM text generation.
Step 1:
  - The decode or what's called prefill, where it takes your input context and has to essentially do a forward pass over over every token in there.
Step 2:
  - Accessing the KV cache from the earlier parts of the conversation and using it to ore quickly generate the next answers.
A lot of LLM APIs use quantization methods to deal with Memory requirements.
Hugging Face Agents API has the LLM generate Python code directly, which then it runs in a limited sandboxed interpreter.

 VectorDB (Retrieval Augmented Generation) Pros:
  - Adds documents/images to database
  - Prepend search results to user's prompt
  - Summarize and archive chat history
  - Long-term memory for LLM's
  - High-dimensional multimodal embeddings generated by other Transformers (CLIP, BERT, ImageBind, etc..)
  - FAISS, RAFT libraries for CUDA-optimized indexing and ANN/KNN nearest-neighbor ranking.

  Riva (Streaming ASR/TTS) Pros:
    - Audio Transformers + TensorRT
    - ASR models in 15 languages
    - 12 built-in voices + SSML expressions
    - Fine-tune models with NeMo
    - Low overhead / GPU utilization

You can run these without any internet connection once you download the containers or have your application built.
